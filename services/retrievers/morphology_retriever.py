# services/retrievers/morphology_retriever.py
"""
smart_exact_match()   â€“ resilient, zero-dependency matcher

  â€¢ Uses utils.arabic.normalize()  âŸ¶  unifies hamza/alif & removes diacritics
  â€¢ Generates spelling *variants* for both query and token strings.
  â€¢ A match succeeds if *any* variant of the query equals *any*
    variant of the token.

This solves:
    â€“ Ø§Ù„Ø¹Ù‡Ù†  â†”  ÙƒØ§Ù„Ø¹Ù‡Ù†
    â€“ ÙƒØ§Ù„Ø¹Ù‡Ù†  â†”  Ø§Ù„Ø¹Ù‡Ù†
    â€“ ÙˆÙØ¯Ø§    â†”  ÙˆÙØ¯
    â€“ dagger-alif and other harakÄt
without mutilating the underlying data.
"""
from __future__ import annotations

import json
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple

from utils.arabic import normalize, strip_diacritics
from utils.paths import MORPHOLOGY_FILE


# â”€â”€ helper: load / group tokens into full QurÊ¾Änic "words" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _group_key(tok: Dict) -> tuple:
    return tok["surah"], tok["ayah"], tok["word_index"]


def _concat(tokens: List[Dict]) -> str:
    """Concatenate raw tokens preserving order in the verseâ€word."""
    return "".join(t["token"] for t in sorted(tokens, key=lambda x: x["token_index"]))


# â”€â”€ spelling-variant generator â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_PROCLITICS: Set[str] = {"Ùƒ", "Ù", "Ø¨", "Ù„", "Ø³", "Ùˆ"}  # keep Ùˆ as *optional* only

# Imperfect-verb prefixes that can follow the future particle Â«Ø³Â»
_IMPF_PREFIXES: Set[str] = {"Ø£", "Ø¥", "Ø¢", "ÙŠ", "Øª", "Ù†"}

def _variants(word: str, root_initial: str | None = None) -> Set[str]:
    """
    Generate a small set of orthographic variants that differ by:
        â€¢ leading proclitic (one char from _PROCLITICS)
        â€¢ leading definite article Â«Ø§Ù„Â»
        â€¢ trailing case-seat Â«Ø§Â»
        â€¢ tanween (Ù‹ Ù ÙŒ)
    The word itself is always included.
    """
    
    forms: Set[str] = {word}
    

    # â”€â”€ 1. optional removal of a *single* proclitic  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # We apply tighter heuristics so that we never strip a letter that is
    # actually the first radical of the root (e.g. Â«Ø³Â» in Â«Ø³Ø§Ø¨Ù‚Â»).
    if len(word) > 3 and word[0] in _PROCLITICS:
        # Skip if the letter seems to belong to the root itself
        if root_initial and word[0] == root_initial:
            pass  # do NOT remove â€“ keeps integrity of root-initial
        else:
            # Special case Â«Ø³+Â» : only future-particle when followed by an
            # imperfect prefix.
            if word[0] == "Ø³":
                if word[1] in _IMPF_PREFIXES:
                    forms.add(word[1:])
            else:
                forms.add(word[1:])
        

    tmp = set(forms)  # snapshot before next rules

    # remove definite article
    for w in tmp:
        if w.startswith("Ø§Ù„") and len(w) > 3:
            forms.add(w[2:])
            

    # remove tanween and trailing alif
    for w in list(forms):
        # Remove tanween
        if "Ù‹" in w:
            forms.add(w.replace("Ù‹", ""))
        if "Ù" in w:
            forms.add(w.replace("Ù", ""))
        if "ÙŒ" in w:
            forms.add(w.replace("ÙŒ", ""))
        # Remove trailing alif
        if w.endswith("Ø§") and len(w) > 3:
            forms.add(w[:-1])
        

    return forms


# â”€â”€ main public function â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def smart_exact_match(
    query_word: str,
    morphology_path: str | Path = MORPHOLOGY_FILE,
) -> Tuple[Optional[List[Dict]], str]:
    """
    Lookup tolerant to:
      â€¢ diacritics / dagger-alif
      â€¢ definite article
      â€¢ proclitics: Ùƒ Ù Ø¨ Ù„ Ø³ Ùˆ
      â€¢ final tanwÄ«n seat Â«Ø§Â»
    Returns (token_list, note) or (None, error note)
    """
    f = Path(morphology_path)
    if not f.exists():
        return None, f"â— morphology file not found: {f}"

    # Normalize the query word by removing diacritics
    q_norm = normalize(query_word)

    # 2ï¸âƒ£  read once, group tokens
    token_groups: dict[tuple, list] = {}
    with f.open(encoding="utf-8") as fh:
        for line in fh:
            tok = json.loads(line)
            token_groups.setdefault(_group_key(tok), []).append(tok)

    # 3ï¸âƒ£  iterate groups, compare variants sets
    for key, toks in token_groups.items():
        # First check if the lemma matches (most strict)
        lemma = toks[0].get("lemma", "")
        if lemma:
            lemma_norm = normalize(lemma)
  
            if lemma_norm == q_norm:
                s, a, w = key
                print(f"ğŸ” [DEBUG] Found exact lemma match: '{query_word}' in S{s}:A{a}, word_index={w}")
                return toks, f"âœ… Exact lemma match: S{s}:A{a}, word_index={w}"

        # Then check if the surface form of the **whole QurÊ¾Änic word** matches
        # A QurÊ¾Änic "word" may consist of multiple tokens (e.g.
        # Â«Ø£ÙØ¨ÙØ§Ù†ÙØ§Â» â†’ ["Ø£ÙØ¨ÙØ§", "Ù†ÙØ§"]).  We therefore concatenate the tokens
        # first and compare against the query word â€“ after normalisation â€“ using
        # the spelling-variant helper.

        surface = _concat(toks)
        surface_norm = normalize(surface)

        # Generate small variant sets for robust matching
        query_forms   = _variants(q_norm)
        root_initial  = (toks[0].get("root") or "")[:1] if toks[0].get("root") else None
        surface_forms = _variants(surface_norm, root_initial)

        if query_forms & surface_forms:
            s, a, w = key
            print(f"ğŸ” [DEBUG] Found surface match: '{query_word}' in S{s}:A{a}, word_index={w}")
            return toks, f"âœ… Surface match: S{s}:A{a}, word_index={w}"

        # Finally, if surface matching failed, fall back to root comparison
        root = toks[0].get("root", "")
        if root:
            root_norm = normalize(root)
            if root_norm == q_norm:
                s, a, w = key
                print(f"ğŸ” [DEBUG] Found exact root match: '{query_word}' in S{s}:A{a}, word_index={w}")
                return toks, f"âœ… Exact root match: S{s}:A{a}, word_index={w}"

    # 4ï¸âƒ£  not found
    return None, (
        f"The word Â«{query_word}Â» was not located in the morphology database "
        "after normalisation and variant matching."
    )


# â”€â”€ quick self-test â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    tests = ["Ø§Ù„Ø¹Ù‡Ù†", "ÙƒØ§Ù„Ø¹Ù‡Ù†", "Ø¹Ù‡Ù†", "ÙˆÙØ¯", "ÙˆÙØ¯Ø§"]
    for w in tests:
        res, note = smart_exact_match(w)
        print(f"{w:<6} â†’ {'âœ…' if res else 'âŒ'}  {note}")